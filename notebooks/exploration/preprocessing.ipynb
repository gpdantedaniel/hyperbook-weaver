{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54684d23",
   "metadata": {},
   "source": [
    "# Building contrastive examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "558c55b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\hyperbook-weaver\\venv\\Lib\\site-packages\\spacy\\language.py:2195: FutureWarning: Possible set union at position 6328\n",
      "  deserializers[\"tokenizer\"] = lambda p: self.tokenizer.from_disk(  # type: ignore[union-attr]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "df = pd.read_csv('../../data/raw/hyperbook.csv', index_col='url')\n",
    "nlp = spacy.load('en_core_sci_md')  # This can take a while"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "79d408b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "def group_intersecting_sets(sets_list):\n",
    "    # Keep track of which sets have been visited\n",
    "    visited = [False] * len(sets_list)\n",
    "    result = []\n",
    "\n",
    "    for i, s in enumerate(sets_list):\n",
    "        if visited[i]:\n",
    "            continue\n",
    "\n",
    "        # Start a new component\n",
    "        group = []\n",
    "        queue = deque([i])\n",
    "        visited[i] = True\n",
    "\n",
    "        while queue:\n",
    "            idx = queue.popleft()\n",
    "            group.append(sets_list[idx])\n",
    "\n",
    "            for j, other_set in enumerate(sets_list):\n",
    "                if not visited[j] and not sets_list[idx].isdisjoint(other_set):\n",
    "                    visited[j] = True\n",
    "                    queue.append(j)\n",
    "\n",
    "        result.append(group)\n",
    "\n",
    "    return result\n",
    "\n",
    "authors = [set(authors.split(',')) for authors in df['authors']]\n",
    "triplets = group_intersecting_sets(authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "475b6dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "FIG_RE = re.compile(r\"^(fig(ure)?|table|eq(uation)?)\\s*\\d+\", re.I)\n",
    "CIT_RE = re.compile(r\"^\\[?\\d{1,3}\\]?$\")\n",
    "\n",
    "def is_good(sent: str) -> bool:\n",
    "    \"\"\"\n",
    "    Validates a sentence and returns a boolean.\n",
    "    Excludes figures, citations, section heads, and\n",
    "    sentences with alphabetic ratios less than 40%,\n",
    "    less than 4 tokens or more than 128 tokens.\n",
    "    \"\"\"\n",
    "    text = sent.strip()\n",
    "    # Exclude figures and citations\n",
    "    if FIG_RE.match(text) or CIT_RE.match(text): return False\n",
    "    # Exclude section heads\n",
    "    if text.isupper() and len(text.split()) <= 6: return False\n",
    "    doc = nlp(text)\n",
    "    if len(doc) < 4 or len(doc) > 128: return False\n",
    "    # Ensure alphabetic ratio greater than 40%\n",
    "    alpha_ratio = sum(t.is_alpha for t in doc)/len(doc) \n",
    "    return alpha_ratio >= 0.4\n",
    "\n",
    "def clean_paper(content):\n",
    "    doc = nlp(content)\n",
    "    for sent in doc.sents:\n",
    "        if is_good(sent.text):\n",
    "            yield sent.text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "6bcbd15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "organism_maps = {}\n",
    "\n",
    "papers = list(df['content'])\n",
    "\n",
    "for pid in range(len(papers)):\n",
    "    for organism_id, triplet in enumerate(triplets):\n",
    "        if authors[pid] in triplet:\n",
    "            organism_maps[pid] = organism_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b2d385",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 207/207 [10:55<00:00,  3.17s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "sentences, positives, paper_ids = [], [], []\n",
    "\n",
    "for pid, content in tqdm(list(enumerate(papers))):\n",
    "    sents = list(clean_paper(content))\n",
    "    # Last sentence has no positive\n",
    "    for i in range(len(sents) - 1):\n",
    "        sentences.append(sents[i])\n",
    "        positives.append(sents[i + 1])\n",
    "        paper_ids.append(pid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "63eeae0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "organism_ids = [organism_maps[pid] for pid in paper_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "0967d333",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "data = {\n",
    "    'sentences': sentences, \n",
    "    'paper_ids': paper_ids, \n",
    "    'organism_ids': organism_ids, \n",
    "    'positives': positives\n",
    "}\n",
    "\n",
    "output_path = '../../data/processed/indexed_sentences.json'\n",
    "\n",
    "with open(output_path, 'w') as file:\n",
    "    json.dump(data, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e39af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "output_path = '../../data/processed/indexed_sentences.json'\n",
    "\n",
    "with open(output_path) as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "sentences, positives, paper_ids, organism_ids = data['sentences'], data['positives'], data['paper_ids'], data['organism_ids']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3202dfc",
   "metadata": {},
   "source": [
    "## Hard-negative mining for InfoNCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "8e53ea4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model_name = 'Qwen/Qwen3-Embedding-0.6B'\n",
    "model = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "9f30145a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d5f8b2bfb8e411395b30effbd6385b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embeddings = model.encode(\n",
    "    sentences, \n",
    "    batch_size=64, \n",
    "    normalize_embeddings=True,  # Normalizes into unit vectors\n",
    "    show_progress_bar=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "39a11322",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "d = embeddings.shape[1]\n",
    "index = faiss.IndexFlatIP(d)    # Cosine because vectors are unit-norm\n",
    "index.add(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "30f813f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51146, 1024)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "69f13732",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mine_negatives(i, k=3, lower=0.6, upper=0.9):\n",
    "    # Get the top 50 candidates\n",
    "    D, I = index.search(embeddings[i:i+1], 50)\n",
    "    hard = []\n",
    "\n",
    "    for j, score in zip(I[0], D[0]):\n",
    "        if j == i: continue\n",
    "        if paper_ids[j] == paper_ids[i]: continue\n",
    "        if organism_ids[j] == organism_ids[i]: continue\n",
    "        if lower <= score <= upper: hard.append(sentences[j])\n",
    "        if len(hard) == k: break\n",
    "\n",
    "    return hard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "5564ba63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51146/51146 [12:18<00:00, 69.29it/s]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "output_path = '../../data/processed/hyperbook_infonce.jsonl'\n",
    "\n",
    "with open(file=output_path, mode='w', encoding='utf-8') as file:\n",
    "    for i, (anchor, positive) in tqdm(list(enumerate(zip(sentences, positives)))):\n",
    "        negatives = mine_negatives(i)\n",
    "        file.write(json.dumps({\n",
    "            'query': anchor,\n",
    "            'response': positive,\n",
    "            'rejected_response': negatives\n",
    "        }, ensure_ascii=False) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "c6ed0948",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "data = []\n",
    "\n",
    "with open('../../data/processed/hyperbook_infonce.jsonl', 'r', encoding='utf-8') as json_file:\n",
    "    json_list = list(json_file)\n",
    "\n",
    "for json_str in json_list:\n",
    "    data.append(json.loads(json_str))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
