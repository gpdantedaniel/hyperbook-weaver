{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55903b4e",
   "metadata": {},
   "source": [
    "## Domain Adaptation\n",
    "\n",
    "Adapting Qwen3-Embedding-0.6B to the Hyperbook's vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc7c45bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:swift] rank: -1, local_rank: -1, world_size: 1, local_world_size: 1\n",
      "[INFO:swift] Downloading the model from ModelScope Hub, model_id: Qwen/Qwen3-Embedding-0.6B\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model from https://www.modelscope.cn to directory: C:\\Users\\dante\\.cache\\modelscope\\hub\\models\\Qwen\\Qwen3-Embedding-0.6B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:modelscope] Target directory already exists, skipping creation.\n",
      "[INFO:swift] Loading the model using model_dir: C:\\Users\\dante\\.cache\\modelscope\\hub\\models\\Qwen\\Qwen3-Embedding-0___6B\n",
      "[INFO:swift] Setting torch_dtype: torch.bfloat16\n",
      "[INFO:swift] Setting args.lazy_tokenize: False\n",
      "[INFO:swift] Setting args.dataloader_num_workers: 0\n",
      "[INFO:swift] output_dir: d:\\Projects\\hyperbook-weaver\\models\\v0-20250625-133710\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from swift.llm import sft_main, TrainArguments\n",
    "\n",
    "os.environ['USE_LIBUV'] = '0'\n",
    "os.environ['NPROC_PER_NODE'] = '8'\n",
    "\n",
    "args = TrainArguments(\n",
    "    model='Qwen/Qwen3-Embedding-0.6B',\n",
    "    model_type='qwen3_emb',\n",
    "    task_type='embedding',\n",
    "    train_type='full',\n",
    "    dataset='../../data/processed/hyperbook_infonce.jsonl',\n",
    "    split_dataset_ratio=0.05,\n",
    "    output_dir='../../models',\n",
    "    eval_strategy='steps',\n",
    "    eval_steps=20,\n",
    "    num_train_epochs=5,\n",
    "    save_steps=20,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=6e-6,\n",
    "    loss_type='infonce',\n",
    "    label_names='labels',\n",
    "    dataloader_drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77780249",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:swift] Global seed set to 42\n",
      "[INFO:swift] args: TrainArguments(\n",
      "_n_gpu=-1,\n",
      "acc_steps=1,\n",
      "acc_strategy=token,\n",
      "accelerator_config={'dispatch_batches': False},\n",
      "adafactor=False,\n",
      "adalora_beta1=0.85,\n",
      "adalora_beta2=0.85,\n",
      "adalora_deltaT=1,\n",
      "adalora_init_r=12,\n",
      "adalora_orth_reg_weight=0.5,\n",
      "adalora_target_r=8,\n",
      "adalora_tfinal=0,\n",
      "adalora_tinit=0,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.95,\n",
      "adam_epsilon=1e-08,\n",
      "adapter_act=gelu,\n",
      "adapter_length=128,\n",
      "adapters=[],\n",
      "add_version=True,\n",
      "agent_template=None,\n",
      "aligner_lr=None,\n",
      "attn_impl=None,\n",
      "auto_find_batch_size=False,\n",
      "average_tokens_across_devices=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=True,\n",
      "bf16_full_eval=False,\n",
      "bnb_4bit_compute_dtype=torch.bfloat16,\n",
      "bnb_4bit_quant_storage=None,\n",
      "bnb_4bit_quant_type=nf4,\n",
      "bnb_4bit_use_double_quant=True,\n",
      "boft_block_num=0,\n",
      "boft_block_size=4,\n",
      "boft_dropout=0.0,\n",
      "boft_n_butterfly_factor=1,\n",
      "channels=None,\n",
      "check_model=True,\n",
      "ckpt_dir=None,\n",
      "columns={},\n",
      "create_checkpoint_symlink=False,\n",
      "custom_dataset_info=[],\n",
      "custom_register_path=[],\n",
      "data_seed=42,\n",
      "dataloader_drop_last=True,\n",
      "dataloader_num_workers=None,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "dataset=../../data/processed/hyperbook_infonce.jsonl,\n",
      "dataset_num_proc=1,\n",
      "dataset_shuffle=True,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=18000000,\n",
      "debug=,\n",
      "deepspeed=None,\n",
      "device_map=None,\n",
      "disable_tqdm=None,\n",
      "do_eval=False,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "download_mode=reuse_dataset_if_exists,\n",
      "eval_accumulation_steps=None,\n",
      "eval_datasets=[],\n",
      "eval_datasets_args=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_generation_config=None,\n",
      "eval_limit=None,\n",
      "eval_on_start=False,\n",
      "eval_steps=20,\n",
      "eval_strategy=steps,\n",
      "eval_use_evalscope=False,\n",
      "eval_use_gather_object=False,\n",
      "external_plugins=[],\n",
      "fourier_n_frequency=2000,\n",
      "fourier_scaling=300.0,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "freeze_aligner=True,\n",
      "freeze_llm=False,\n",
      "freeze_parameters=[],\n",
      "freeze_parameters_ratio=0.0,\n",
      "freeze_parameters_regex=None,\n",
      "freeze_vit=True,\n",
      "fsdp=,\n",
      "fsdp_config=None,\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_num=1,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "galore_cos_threshold=0.4,\n",
      "galore_gamma_proj=2,\n",
      "galore_optim_per_parameter=False,\n",
      "galore_proj_bits=4,\n",
      "galore_proj_group_size=256,\n",
      "galore_proj_quant=False,\n",
      "galore_proj_type=std,\n",
      "galore_quantization=False,\n",
      "galore_queue_size=5,\n",
      "galore_rank=128,\n",
      "galore_scale=1.0,\n",
      "galore_target_modules=None,\n",
      "galore_update_proj_gap=50,\n",
      "galore_with_embedding=False,\n",
      "generation_config=None,\n",
      "generation_max_length=None,\n",
      "generation_num_beams=None,\n",
      "gradient_accumulation_steps=4,\n",
      "gradient_checkpointing=True,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=False,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hqq_axis=None,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=None,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_args_error=False,\n",
      "ignore_data_skip=False,\n",
      "include_for_metrics=[],\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "init_strategy=None,\n",
      "init_weights=True,\n",
      "interleave_prob=None,\n",
      "jit_mode_eval=False,\n",
      "label_names=labels,\n",
      "label_smoothing_factor=0.0,\n",
      "lazy_tokenize=False,\n",
      "learning_rate=6e-06,\n",
      "length_column_name=length,\n",
      "lisa_activated_layers=0,\n",
      "lisa_step_interval=20,\n",
      "llamapro_num_groups=None,\n",
      "llamapro_num_new_blocks=4,\n",
      "load_args=False,\n",
      "load_best_model_at_end=False,\n",
      "load_data_args=False,\n",
      "load_from_cache_file=True,\n",
      "local_rank=-1,\n",
      "local_repo_path=None,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=d:\\Projects\\hyperbook-weaver\\models\\v0-20250625-133710\\runs,\n",
      "logging_first_step=True,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=5,\n",
      "logging_strategy=steps,\n",
      "logprobs=False,\n",
      "lora_alpha=32,\n",
      "lora_bias=none,\n",
      "lora_dropout=0.05,\n",
      "lora_dtype=None,\n",
      "lora_ga_batch_size=2,\n",
      "lora_ga_direction=ArB2r,\n",
      "lora_ga_iters=2,\n",
      "lora_ga_max_length=1024,\n",
      "lora_ga_scale=stable,\n",
      "lora_ga_stable_gamma=16,\n",
      "lora_modules=[],\n",
      "lora_rank=8,\n",
      "lorap_lr_ratio=None,\n",
      "loss_scale=default,\n",
      "loss_type=infonce,\n",
      "lr_scheduler_kwargs=None,\n",
      "lr_scheduler_type=cosine,\n",
      "max_epochs=None,\n",
      "max_grad_norm=1.0,\n",
      "max_length=None,\n",
      "max_memory={},\n",
      "max_new_tokens=64,\n",
      "max_pixels=None,\n",
      "max_steps=-1,\n",
      "metric=None,\n",
      "metric_for_best_model=loss,\n",
      "metric_warmup_step=0,\n",
      "model=Qwen/Qwen3-Embedding-0.6B,\n",
      "model_author=None,\n",
      "model_kwargs={},\n",
      "model_name=None,\n",
      "model_revision=None,\n",
      "model_type=qwen3_emb,\n",
      "modules_to_save=[],\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "norm_bbox=None,\n",
      "num_beams=1,\n",
      "num_labels=None,\n",
      "num_train_epochs=5,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "optimizer=None,\n",
      "output_dir=d:\\Projects\\hyperbook-weaver\\models\\v0-20250625-133710,\n",
      "overwrite_output_dir=False,\n",
      "packing=False,\n",
      "packing_cache=None,\n",
      "padding_free=False,\n",
      "padding_side=right,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=4,\n",
      "per_device_train_batch_size=4,\n",
      "predict_with_generate=False,\n",
      "prediction_loss_only=False,\n",
      "problem_type=None,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "quant_bits=None,\n",
      "quant_method=None,\n",
      "ray_scope=last,\n",
      "reft_args=None,\n",
      "reft_intervention_type=LoreftIntervention,\n",
      "reft_layer_key=None,\n",
      "reft_layers=None,\n",
      "reft_rank=4,\n",
      "remove_unused_columns=True,\n",
      "repetition_penalty=None,\n",
      "report_to=['tensorboard'],\n",
      "response_prefix=None,\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "resume_only_model=False,\n",
      "rope_scaling=None,\n",
      "run_name=d:\\Projects\\hyperbook-weaver\\models\\v0-20250625-133710,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=20,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sequence_parallel_size=1,\n",
      "shuffle_buffer_size=1000,\n",
      "skip_memory_metrics=True,\n",
      "sortish_sampler=False,\n",
      "split_dataset_ratio=0.05,\n",
      "stop_words=[],\n",
      "stopping_strategy=first_exhausted,\n",
      "stream=False,\n",
      "streaming=False,\n",
      "strict=False,\n",
      "swanlab_exp_name=None,\n",
      "swanlab_mode=cloud,\n",
      "swanlab_project=None,\n",
      "swanlab_token=<SWANLAB_TOKEN>,\n",
      "swanlab_workspace=None,\n",
      "system=None,\n",
      "target_modules=['all-linear'],\n",
      "target_regex=None,\n",
      "task_type=embedding,\n",
      "temperature=0.0,\n",
      "template=qwen3_emb,\n",
      "template_backend=swift,\n",
      "tf32=None,\n",
      "top_k=None,\n",
      "top_logprobs=None,\n",
      "top_p=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_dtype=torch.bfloat16,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "train_dataloader_shuffle=True,\n",
      "train_type=full,\n",
      "trainable_parameters=[],\n",
      "trainable_parameters_regex=None,\n",
      "truncation_strategy=delete,\n",
      "tuner_backend=peft,\n",
      "use_chat_template=True,\n",
      "use_cpu=False,\n",
      "use_dora=False,\n",
      "use_galore=False,\n",
      "use_hf=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_liger_kernel=False,\n",
      "use_logits_to_keep=None,\n",
      "use_mps_device=False,\n",
      "use_rslora=False,\n",
      "use_swift_lora=False,\n",
      "val_dataset=[],\n",
      "val_dataset_shuffle=False,\n",
      "vera_d_initial=0.1,\n",
      "vera_dropout=0.0,\n",
      "vera_projection_prng_key=0,\n",
      "vera_rank=256,\n",
      "vit_gradient_checkpointing=None,\n",
      "vit_lr=None,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.1,\n",
      "zero_hpz_partition_size=None,\n",
      ")\n",
      "[INFO:swift] Downloading the model from ModelScope Hub, model_id: Qwen/Qwen3-Embedding-0.6B\n"
     ]
    }
   ],
   "source": [
    "sft_main(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
